{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1jVQHM_bkKg40OrvnplaGqPcGdjcZkeT7","authorship_tag":"ABX9TyMUmjpHHE+p3NUknzK9riN9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Kg5T4x2Yr3Te"},"outputs":[],"source":["import torch                                             # Deep learning library\n","from torch import nn                                     # Neural network functions\n","from torchvision import transforms                       # Data transformation functions\n","from torchvision.transforms import InterpolationMode     # Image interpolation methods\n","\n","import numpy as np                              # Numerical computation library\n","import matplotlib.pyplot as plt                 # Plotting library\n","import matplotlib.image as mpimg                # Image operations library\n","from PIL import Image                           # Image processing library\n","from skimage.color import rgb2lab, lab2rgb      # Color space conversion functions"]},{"cell_type":"code","source":["def get_default_device():\n","    \"\"\"Pick GPU if available, else CPU\"\"\"\n","    if torch.cuda.is_available():\n","        return torch.device('cuda')\n","    else:\n","        return torch.device('cpu')\n","\n","def to_device(data, device):\n","    \"\"\"Move tensor(s) to chosen device\"\"\"\n","    if isinstance(data, (list,tuple)):\n","        return [to_device(x, device) for x in data]\n","    return data.to(device, non_blocking=True)"],"metadata":{"id":"XNs0N5ASg5fq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Generator"],"metadata":{"id":"Tqti-TACNECo"}},{"cell_type":"code","source":["class Block(nn.Module):\n","    def __init__(self, in_channels, out_channels, down=True, act=\"relu\"):\n","        \"\"\"\n","        Block module for the generator network.\n","\n","        Args:\n","            in_channels (int): Number of input channels.\n","            out_channels (int): Number of output channels.\n","            down (bool): Flag indicating whether downsampling should be applied.\n","            act (str): Activation function to use (\"relu\" or \"leaky\").\n","        \"\"\"\n","        super(Block, self).__init__()\n","        # Define convolutional layers with optional downsampling or upsampling\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False) if down\n","            else nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU() if act == \"relu\" else nn.LeakyReLU(0.2)\n","        )\n","        self.down = down\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass of the block module.\n","\n","        Args:\n","            x (torch.Tensor): Input tensor.\n","\n","        Returns:\n","            torch.Tensor: Output tensor.\n","        \"\"\"\n","        x = self.conv(x)\n","        return x\n","\n","\n","class Generator(nn.Module):\n","    def __init__(self, in_channels=1, features=64):\n","        \"\"\"\n","        Generator network for image-to-image translation.\n","\n","        Args:\n","            in_channels (int): Number of input channels (default: 1).\n","            features (int): Number of features in the network (default: 64).\n","        \"\"\"\n","        super().__init__()\n","        # Initial downsampling layer\n","        self.initial_down = nn.Sequential(\n","            nn.Conv2d(in_channels, features, 4, 2, 1, bias=False),\n","            nn.LeakyReLU(0.2),\n","        )\n","\n","        # Residual blocks\n","        self.res1 = nn.Sequential(\n","            nn.Conv2d(features, features, 3, 1, 1, bias=False),\n","            nn.BatchNorm2d(features),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(features, features, 3, 1, 1, bias=False),\n","            nn.BatchNorm2d(features)\n","        )\n","\n","        self.res2 = nn.Sequential(\n","            nn.Conv2d(features, features, 3, 1, 1, bias=False),\n","            nn.BatchNorm2d(features),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(features, features, 3, 1, 1, bias=False),\n","            nn.BatchNorm2d(features)\n","        )\n","\n","        # Downsample 1\n","        self.res3 = nn.Sequential(\n","            Block(features, features * 2, down=True, act=\"leaky\"),\n","            nn.Conv2d(features * 2, features * 2, 3, 1, 1, bias=False),\n","            nn.BatchNorm2d(features * 2)\n","        )\n","        self.downsample1 = nn.Sequential(\n","            nn.Conv2d(features, features * 2, 1, 2, bias=False),\n","            nn.BatchNorm2d(features * 2)\n","        )\n","\n","        # Residual block\n","        self.res4 = nn.Sequential(\n","            nn.Conv2d(features * 2, features * 2, 3, 1, 1, bias=False),\n","            nn.BatchNorm2d(features * 2),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(features * 2, features * 2, 3, 1, 1, bias=False),\n","            nn.BatchNorm2d(features * 2)\n","        )\n","\n","        # Downsample 2\n","        self.down2 = nn.Sequential(\n","            Block(features * 2, features * 4, down=True, act=\"leaky\"),\n","            nn.Conv2d(features * 4, features * 4, 3, 1, 1, bias=False),\n","            nn.BatchNorm2d(features * 4)\n","        )\n","        self.downsample2 = nn.Sequential(\n","            nn.Conv2d(features * 2, features * 4, 1, 2, bias=False),\n","            nn.BatchNorm2d(features * 4)\n","        )\n","\n","        # Residual block\n","        self.res5 = nn.Sequential(\n","            nn.Conv2d(features * 4, features * 4, 3, 1, 1, bias=False),\n","            nn.BatchNorm2d(features * 4),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(features * 4, features * 4, 3, 1, 1, bias=False),\n","            nn.BatchNorm2d(features * 4)\n","        )\n","\n","        # Downsample 3\n","        self.downsample3 = nn.Sequential(\n","            nn.Conv2d(features * 8, features * 8, 1, 2, bias=False),\n","            nn.BatchNorm2d(features * 8)\n","        )\n","        self.down3 = Block(features * 4, features * 8, down=True, act=\"leaky\")\n","\n","        # Downsample 4\n","        self.downsample4 = nn.Sequential(\n","            nn.Conv2d(features * 8, features * 8, 1, 2, bias=False),\n","            nn.BatchNorm2d(features * 8)\n","        )\n","        self.down4 = Block(features * 8, features * 8, down=True, act=\"leaky\")\n","\n","        # Downsample 5\n","        self.downsample5 = nn.Sequential(\n","            nn.Conv2d(features * 4, features * 8, 1, 2, bias=False),\n","            nn.BatchNorm2d(features * 8)\n","        )\n","\n","        self.down5 = Block(features * 8, features * 8, down=True, act=\"leaky\")\n","        self.down6 = Block(features * 8, features * 8, down=True, act=\"leaky\")\n","\n","        # Bottleneck\n","        self.bottleneck = nn.Sequential(\n","            nn.Conv2d(features * 8, features * 8, 4, 2, 1),\n","            nn.ReLU()\n","        )\n","\n","        # Upsampling blocks\n","        self.up1 = Block(features * 8, features * 8, down=False, act=\"relu\")\n","        self.up2 = nn.Sequential(\n","            Block(features * 8 * 2, features * 8, down=False, act=\"relu\"),\n","            nn.Dropout(0.5)\n","        )\n","        self.up3 = nn.Sequential(\n","            Block(features * 8 * 2, features * 8, down=False, act=\"relu\"),\n","            nn.Dropout(0.5)\n","        )\n","        self.up4 = nn.Sequential(\n","            Block(features * 8 * 2, features * 8, down=False, act=\"relu\"),\n","            nn.Dropout(0.5)\n","        )\n","        self.up5 = Block(features * 8 * 2, features * 4, down=False, act=\"relu\")\n","        self.up6 = Block(features * 4 * 2, features * 2, down=False, act=\"relu\")\n","        self.up7 = Block(features * 2 * 2, features, down=False, act=\"relu\")\n","\n","        # Final output layer\n","        self.final_up = nn.Sequential(\n","            nn.ConvTranspose2d(features * 2, 2, kernel_size=4, stride=2, padding=1),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass of the generator network.\n","\n","        Args:\n","            x (torch.Tensor): Input tensor.\n","\n","        Returns:\n","            torch.Tensor: Output tensor.\n","        \"\"\"\n","        # Downsampling\n","        d1 = self.initial_down(x)\n","        d2 = self.res1(d1)\n","        d3 = self.res2(d2) + d1\n","        d4 = self.res3(d3) + self.downsample1(d3)  #downsample\n","        d5 = self.res4(d4) + d4  #downsample\n","        d6 = self.down2(d5) + self.downsample2(d5)\n","        d7 = self.res5(d6) + d6\n","        d8 = self.down3(d7) + self.downsample5(d7)\n","        d9 = self.down4(d8) + self.downsample3(d8)\n","        d10 = self.down5(d9)\n","        d11 = self.down6(d10) + self.downsample4(d10)\n","        # Bottleneck\n","        bottleneck = self.bottleneck(d11)\n","        # Upsampling blocks\n","        up1 = self.up1(bottleneck)\n","        up2 = self.up2(torch.cat([up1, d11], 1))\n","        up3 = self.up3(torch.cat([up2, d10], 1))\n","        up4 = self.up4(torch.cat([up3, d9], 1))\n","        up5 = self.up5(torch.cat([up4, d8], 1))\n","        up6 = self.up6(torch.cat([up5, d7], 1))\n","        up7 = self.up7(torch.cat([up6, d5], 1))\n","        # Final output\n","        return self.final_up(torch.cat([up7, d3], 1))"],"metadata":{"id":"bJzJBEN-UK-o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Converting LAB images to RGB"],"metadata":{"id":"ZFhscJnKrVrD"}},{"cell_type":"code","source":["def lab_to_rgb(L, ab):\n","    \"\"\"\n","    Converts a batch of LAB images to RGB format.\n","\n","    Args:\n","        L (torch.Tensor): L channel of LAB images with shape (batch_size, 1, height, width).\n","        ab (torch.Tensor): ab channels of LAB images with shape (batch_size, 2, height, width).\n","\n","    Returns:\n","        np.ndarray: Array of RGB images with shape (batch_size, height, width, 3).\n","\n","    Note:\n","        - Input L values are scaled from [-1, 1] to [0, 100] range.\n","        - Input ab values are scaled from [-1, 1] to [-128, 128] range.\n","        - The LAB images are converted to RGB using the lab2rgb function from the skimage.color module.\n","    \"\"\"\n","\n","    # Scale L values from [-1, 1] to [0, 100] range\n","    L = (L + 1.) * 50.\n","\n","    # Scale ab values from [-1, 1] to [-128, 128] range\n","    ab = ab * 128.\n","\n","    # Concatenate L and ab channels, permute dimensions, and convert to numpy array\n","    Lab = torch.cat([L, ab], dim=1).permute(0, 2, 3, 1).cpu().numpy()\n","\n","    # Convert LAB images to RGB\n","    rgb_imgs = []\n","    for img in Lab:\n","        img_rgb = lab2rgb(img)\n","        rgb_imgs.append(img_rgb)\n","\n","    # Stack RGB images into a single numpy array\n","    return np.stack(rgb_imgs, axis=0)\n"],"metadata":{"id":"ZYOI2cGuCJTq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Image Plotting"],"metadata":{"id":"QcawF2f7ra0D"}},{"cell_type":"code","source":["def visualize_side(data, fake_imgs, path = None, save=False):\n","    \"\"\"\n","    Visualize grayscale and colorized images side by side and optionally save the figure.\n","\n","    Args:\n","        data (list): List of grayscale image to visualize.\n","        fake_imgs (list): List of colorized image to visualize.\n","        path (str): Path to save the figure (if save is True).\n","        save (bool, optional): Flag indicating whether to save the figure. Defaults to False.\n","    \"\"\"\n","\n","    # Create a new figure with a size of 15x10 inches\n","    fig = plt.figure(figsize=(15, 10))\n","\n","    # Iterate over the range 0 to 1 (for two subplots)\n","    for i in range(2):\n","        # Add a subplot to the figure at position i+1\n","        ax = plt.subplot(1, 2, i + 1)\n","\n","        # Check if the current iteration is for the first subplot\n","        if i == 0:\n","            # Display the grayscale image\n","            ax.imshow(data[0][0].cpu(), cmap='gray')\n","        else:\n","            # Display the colorized image\n","            ax.imshow(fake_imgs[0])\n","\n","        # Turn off the axis labels\n","        ax.axis(\"off\")\n","\n","    # Show the figure\n","    plt.show()\n","\n","    # Save the figure if save is True\n","    if save:\n","      try:\n","        # Save the figure as a PNG image with the provided path\n","        fig.savefig(path + \"_side.png\")\n","        print(\"Figure saved successfully.\")\n","      except Exception as e:\n","        print(\"Error occurred while saving the figure: Path doesn't exist\")"],"metadata":{"id":"UIXTxwlPsxgP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def visualize(fake_imgs):\n","    \"\"\"\n","    Visualize the Colorized Image and optionally save the figure.\n","\n","    Args:\n","        fake_imgs (list): Colorized image to visualize.\n","        path (str): Path to save the figure (if save is True).\n","        save (bool, optional): Flag indicating whether to save the figure. Defaults to False.\n","    \"\"\"\n","\n","    # Create a new figure with a size of 16x16 inches\n","    fig = plt.figure(figsize=(16, 16))\n","\n","    # Add a subplot to the figure\n","    ax = plt.subplot(1, 1, 1)\n","\n","    # Display the colorized image in the subplot\n","    ax.imshow(fake_imgs[0])\n","\n","    # Turn off the axis labels\n","    ax.axis(\"off\")\n","\n","    # Show the figure\n","    plt.show()"],"metadata":{"id":"dYee7eVgsS2e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Transforming Grayscale Images to Colorized Versions"],"metadata":{"id":"UwWvQqX6rSFP"}},{"cell_type":"code","source":["\n","def predict_image(image_path,model,image_save_path=None,save=False,plot=False,plot_side_by_side=False):\n","\n","  \"\"\"\n","    Predicts the colorized version of an input greyscale image using the provided model.\n","\n","    Args:\n","        image_path (str): The path to the input image file.\n","        model: The generator model.\n","        image_save_path (str, optional): The path to save the colorized image. Defaults to None.\n","        save (bool, optional): Whether to save the colorized image. Defaults to False.\n","        plot (bool, optional): Whether to plot the colorized image. Defaults to False.\n","        plot_side_by_side (bool, optional): Whether to plot the input image and colorized image side by side.Defaults to False.\n","  \"\"\"\n","  SIZE=512\n","  torch.cuda.empty_cache()\n","  # Load content and style images\n","  img = Image.open(image_path).convert(\"RGB\")   # Open and convert image to RGB mode\n","  og_size = img.size     # Store original image size\n","  transform = transforms.Resize((SIZE, SIZE), InterpolationMode.BICUBIC)\n","  img = transform(img)      # Resize image to SIZE x SIZE using bicubic interpolation\n","  img = np.array(img)       # Convert image to NumPy array for purpose of converting to LAB\n","  img_lab = rgb2lab(img).astype(\"float32\")   # Convert RGB image to L*a*b color space\n","  img_lab = transforms.ToTensor()(img_lab)   # Convert L*a*b image to PyTorch tensor\n","  L = img_lab[[0], ...] / 50. - 1.      # Normalize L channel to range [-1, 1]\n","  ab = img_lab[[1, 2], ...] / 128.      # Normalize ab channels to range [-1, 1]\n","  L_tensor = L.unsqueeze(0).to(device)  # Add batch dimension and move to device\n","  model.train()       # Set model to training mode as written in the paper\n","  with torch.no_grad():\n","    fake_color = model(L_tensor)       # Generate the ab channel using generator\n","\n","  # Resize L_tensor and fake_color back to original image size\n","  transform_resized = transforms.Resize((og_size[1],og_size[0]), interpolation=InterpolationMode.BICUBIC)\n","  L_tensor = transform_resized(L_tensor.squeeze(0))      # Resize L_tensor using bicubic interpolation\n","  L_tensor = L_tensor.unsqueeze(0)                       # Add back the batch dimension\n","  fake_color = transform_resized(fake_color.squeeze(0))  # Resize ab channel to original Image size using bicubic interpolation\n","  fake_color = fake_color.unsqueeze(0)                   # Add back the batch dimension\n","\n","  fake_imgs = lab_to_rgb(L_tensor, fake_color)        # Combine L_tensor and ab channel to get colorized image\n","  if plot:\n","    visualize(fake_imgs)                      # Display colorized image\n","\n","   # Save the figure if save is True\n","  if save:\n","    try:\n","      # Save the figure as a PNG image with the provided path\n","      fake_imgs_np = np.squeeze(fake_imgs)              # Remove batch dimension from fake_imgs\n","      fake_imgs_np = Image.fromarray((fake_imgs_np * 255).astype(np.uint8))     # Convert to PIL image\n","      fake_imgs_np.save(image_save_path+'.png')            # Save colorized image as PNG\n","      print(\"Colorized Image saved successfully.\")\n","    except Exception as e:\n","      print(\"Error occurred while saving the figure: Path not provided\")\n","\n","  if plot_side_by_side:\n","    visualize_side(L_tensor,fake_imgs,path=image_save_path,save=save)       # Display original and colorized images side by side\n","\n","\n","  torch.cuda.empty_cache()     # Clear CUDA cache\n"],"metadata":{"id":"e1lkfcGQUK1h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = get_default_device()  # Checking which device is available for performing inference\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GU3Ii7shhD5Z","executionInfo":{"status":"ok","timestamp":1689095388068,"user_tz":-330,"elapsed":540,"user":{"displayName":"Modassir Afzal","userId":"04115856496195015671"}},"outputId":"15abec4f-974a-4df9-c19c-a339f07ba4a1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["G = to_device(Generator(features=64),device)      # Loading Generator for image colorization\n","G.load_state_dict(torch.load('/content/drive/MyDrive/trained/G_1_2000000_gan.pth'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OVNO8cy7zcKQ","executionInfo":{"status":"ok","timestamp":1689095400093,"user_tz":-330,"elapsed":10753,"user":{"displayName":"Modassir Afzal","userId":"04115856496195015671"}},"outputId":"58b73c63-f8fd-4350-c00d-43af71265d4a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["predict_image('/content/The-Beatles-Bruce-McBroom-Â©-Apple-Corps-Ltd-696x442.jpg',G,\n","              '/content/sample_data/beatles',\n","              plot=True,\n","              plot_side_by_side=True,save=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1tuVG89cp3kz8mNCKsVlPoWguezT4RGlU"},"id":"n0MeU7vMzcHC","executionInfo":{"status":"ok","timestamp":1689043512916,"user_tz":-330,"elapsed":5548,"user":{"displayName":"Modassir Afzal","userId":"04115856496195015671"}},"outputId":"a319128e-b268-43f5-cab6-660e48be4414"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["predict_image('/content/oxcart-trudge-1953--getty_1413458061.jpg',G,\n","              '/content/sample_data/howrah',\n","              plot=True,\n","              plot_side_by_side=True,save=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1asmHd0Rc9eMZ4JHQuvPydNcBV5uzl6gM"},"id":"Enp3OFL01xJn","executionInfo":{"status":"ok","timestamp":1689044685832,"user_tz":-330,"elapsed":5824,"user":{"displayName":"Modassir Afzal","userId":"04115856496195015671"}},"outputId":"4ca12c68-765d-4155-abdb-be7db0333a05"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["predict_image('/content/calcutta-bus-stand_1413447185_725x725.jpg',G,\n","              '/content/sample_data/howrahbus',\n","              plot=True,\n","              plot_side_by_side=True,save=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1ttrQzSVJTjDefQt3q2Yl2eJHi4B8DNrl"},"id":"pSdmHMLPLVJt","executionInfo":{"status":"ok","timestamp":1689083216224,"user_tz":-330,"elapsed":17231,"user":{"displayName":"Modassir Afzal","userId":"04115856496195015671"}},"outputId":"bca294dc-0097-480f-b201-c5789e432a8e"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["predict_image('/content/chowranghee2-ignouofkolkata_1413451627_725x725.jpg',G,\n","              '/content/sample_data/chowringhee2',\n","              plot=True,\n","              plot_side_by_side=True,save=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1WvjUTLkdluTiwRpHb78NMO8Ptyf-36hY"},"id":"tizDt6D6Ovqy","executionInfo":{"status":"ok","timestamp":1689083722194,"user_tz":-330,"elapsed":10028,"user":{"displayName":"Modassir Afzal","userId":"04115856496195015671"}},"outputId":"b44251ca-2536-41c4-d569-9c2bada023cc"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["predict_image('/content/1593664440_untitled-design-2020-07-02t100133.575.webp',G,\n","              '/content/sample_data/lonkolbus',\n","              plot=True,\n","              plot_side_by_side=True,save=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1Uh9hjK9Gd2iuW_zeQ7AyMJegxlI3ep9T"},"id":"Zhakqd9bLunm","executionInfo":{"status":"ok","timestamp":1689083310879,"user_tz":-330,"elapsed":8522,"user":{"displayName":"Modassir Afzal","userId":"04115856496195015671"}},"outputId":"8347e5a1-0745-4dee-a080-a8ccadc097b0"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["predict_image('/content/rsz_sttreet_scene_1413447327_725x725.jpg',G,\n","              '/content/sample_data/kolkatastreet',\n","              plot=True,\n","              plot_side_by_side=True,save=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1jeWDvwNNO_0JOaJlIXloAzoMSq9_xP24"},"id":"hCMHdfDbNRqz","executionInfo":{"status":"ok","timestamp":1689083292498,"user_tz":-330,"elapsed":9900,"user":{"displayName":"Modassir Afzal","userId":"04115856496195015671"}},"outputId":"6e65bfa9-9e40-4dbf-fad1-6b76b0a9e6b1"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}